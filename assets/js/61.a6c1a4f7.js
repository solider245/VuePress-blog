(window.webpackJsonp=window.webpackJsonp||[]).push([[61],{434:function(s,t,a){"use strict";a.r(t);var n=a(25),e=Object(n.a)({},(function(){var s=this,t=s.$createElement,a=s._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[a("p"),a("div",{staticClass:"table-of-contents"},[a("ul",[a("li",[a("a",{attrs:{href:"#scrapy架构图-绿线是数据流向"}},[s._v("Scrapy架构图(绿线是数据流向)")])]),a("li",[a("a",{attrs:{href:"#scrapy的运作流程"}},[s._v("Scrapy的运作流程")])]),a("li",[a("a",{attrs:{href:"#制作-scrapy-爬虫-一共需要4步："}},[s._v("制作 Scrapy 爬虫 一共需要4步：")])]),a("li",[a("a",{attrs:{href:"#安装"}},[s._v("安装")]),a("ul",[a("li",[a("a",{attrs:{href:"#windows-安装方式"}},[s._v("Windows 安装方式")])]),a("li",[a("a",{attrs:{href:"#ubuntu-安装方式"}},[s._v("Ubuntu 安装方式")])]),a("li",[a("a",{attrs:{href:"#mac-os-安装方式"}},[s._v("Mac OS 安装方式")])])])]),a("li",[a("a",{attrs:{href:"#入门案例"}},[s._v("入门案例")]),a("ul",[a("li",[a("a",{attrs:{href:"#学习目标"}},[s._v("学习目标")])]),a("li",[a("a",{attrs:{href:"#一-新建项目-scrapy-startproject"}},[s._v("一. 新建项目(scrapy startproject)")])]),a("li",[a("a",{attrs:{href:"#二、明确目标-myspider-items-py"}},[s._v("二、明确目标(mySpider/items.py)")])]),a("li",[a("a",{attrs:{href:"#三、制作爬虫-（spiders-itcastspider-py）"}},[s._v("三、制作爬虫 （spiders/itcastSpider.py）")])]),a("li",[a("a",{attrs:{href:"#保存数据"}},[s._v("保存数据")])]),a("li",[a("a",{attrs:{href:"#思考"}},[s._v("思考")])])])])])]),s._v("\nScrapy 是用 Python 实现的一个为了爬取网站数据、提取结构性数据而编写的应用框架。"),a("p"),s._v(" "),a("p",[s._v("Scrapy 常应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。")]),s._v(" "),a("p",[s._v("通常我们可以很简单的通过 Scrapy 框架实现一个爬虫，抓取指定网站的内容或图片。")]),s._v(" "),a("hr"),s._v(" "),a("h2",{attrs:{id:"scrapy架构图-绿线是数据流向"}},[s._v("Scrapy架构图(绿线是数据流向)")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://www.runoob.com/wp-content/uploads/2018/10/8c591d54457bb033812a2b0364011e9c_articlex.png",alt:""}})]),s._v(" "),a("ul",[a("li",[a("p",[a("strong",[s._v("Scrapy Engine(引擎)")]),s._v(": 负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。")])]),s._v(" "),a("li",[a("p",[a("strong",[s._v("Scheduler(调度器)")]),s._v(": 它负责接受引擎发送过来的Request请求，并按照一定的方式进行整理排列，入队，当引擎需要时，交还给引擎。")])]),s._v(" "),a("li",[a("p",[a("strong",[s._v("Downloader（下载器）")]),s._v("：负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spider来处理，")])]),s._v(" "),a("li",[a("p",[a("strong",[s._v("Spider（爬虫）")]),s._v("：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入Scheduler(调度器).")])]),s._v(" "),a("li",[a("p",[a("strong",[s._v("Item Pipeline(管道)")]),s._v("：它负责处理Spider中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方。")])]),s._v(" "),a("li",[a("p",[a("strong",[s._v("Downloader Middlewares（下载中间件）")]),s._v("：你可以当作是一个可以自定义扩展下载功能的组件。")])]),s._v(" "),a("li",[a("p",[a("strong",[s._v("Spider Middlewares（Spider中间件）")]),s._v("：你可以理解为是一个可以自定扩展和操作引擎和Spider中间通信的功能组件（比如进入Spider的Responses;和从Spider出去的Requests）")])])]),s._v(" "),a("h2",{attrs:{id:"scrapy的运作流程"}},[s._v("Scrapy的运作流程")]),s._v(" "),a("p",[s._v("代码写好，程序开始运行...")]),s._v(" "),a("ul",[a("li",[s._v("1 引擎：Hi！Spider, 你要处理哪一个网站？")]),s._v(" "),a("li",[s._v("2 Spider：老大要我处理xxxx.com。")]),s._v(" "),a("li",[s._v("3 引擎：你把第一个需要处理的URL给我吧。")]),s._v(" "),a("li",[s._v("4 Spider：给你，第一个URL是xxxxxxx.com。")]),s._v(" "),a("li",[s._v("5 引擎：Hi！调度器，我这有request请求你帮我排序入队一下。")]),s._v(" "),a("li",[s._v("6 调度器：好的，正在处理你等一下。")]),s._v(" "),a("li",[s._v("7 引擎：Hi！调度器，把你处理好的request请求给我。")]),s._v(" "),a("li",[s._v("8 调度器：给你，这是我处理好的request")]),s._v(" "),a("li",[s._v("9 引擎：Hi！下载器，你按照老大的下载中间件的设置帮我下载一下这个request请求")]),s._v(" "),a("li",[s._v("10 下载器：好的！给你，这是下载好的东西。（如果失败：sorry，这个request下载失败了。然后引擎告诉调度器，这个request下载失败了，你记录一下，我们待会儿再下载）")]),s._v(" "),a("li",[s._v("11 引擎：Hi！Spider，这是下载好的东西，并且已经按照老大的下载中间件处理过了，你自己处理一下（注意！这儿responses默认是交给def parse()这个函数处理的）")]),s._v(" "),a("li",[s._v("12 Spider：（处理完毕数据之后对于需要跟进的URL），Hi！引擎，我这里有两个结果，这个是我需要跟进的URL，还有这个是我获取到的Item数据。")]),s._v(" "),a("li",[s._v("13 引擎：Hi ！管道 我这儿有个item你帮我处理一下！调度器！这是需要跟进URL你帮我处理下。然后从第四步开始循环，直到获取完老大需要全部信息。")]),s._v(" "),a("li",[s._v("14 管道调度器：好的，现在就做！")])]),s._v(" "),a("p",[a("strong",[s._v("注意！只有当调度器中不存在任何request了，整个程序才会停止，（也就是说，对于下载失败的URL，Scrapy也会重新下载。）")])]),s._v(" "),a("hr"),s._v(" "),a("h2",{attrs:{id:"制作-scrapy-爬虫-一共需要4步："}},[s._v("制作 Scrapy 爬虫 一共需要4步：")]),s._v(" "),a("ol",[a("li",[s._v("新建项目 (scrapy startproject xxx)：新建一个新的爬虫项目")]),s._v(" "),a("li",[s._v("明确目标 （编写items.py）：明确你想要抓取的目标")]),s._v(" "),a("li",[s._v("制作爬虫 （spiders/xxspider.py）：制作爬虫开始爬取网页")]),s._v(" "),a("li",[s._v("存储内容 （pipelines.py）：设计管道存储爬取内容")])]),s._v(" "),a("hr"),s._v(" "),a("h2",{attrs:{id:"安装"}},[s._v("安装")]),s._v(" "),a("h3",{attrs:{id:"windows-安装方式"}},[s._v("Windows 安装方式")]),s._v(" "),a("p",[s._v("升级 pip 版本：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("pip install --upgrade pip\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("通过 pip 安装 Scrapy 框架:")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("pip install Scrapy\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("h3",{attrs:{id:"ubuntu-安装方式"}},[s._v("Ubuntu 安装方式")]),s._v(" "),a("p",[s._v("安装非 Python 的依赖:")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("sudo apt-get install python-dev python-pip libxml2-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("通过 pip 安装 Scrapy 框架：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("sudo pip install scrapy\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("h3",{attrs:{id:"mac-os-安装方式"}},[s._v("Mac OS 安装方式")]),s._v(" "),a("p",[s._v("对于Mac OS系统来说，由于系统本身会引用自带的python2.x的库，因此默认安装的包是不能被删除的，但是你用python2.x来安装Scrapy会报错，用python3.x来安装也是报错，我最终没有找到直接安装Scrapy的方法，所以我用另一种安装方式来说一下安装步骤，解决的方式是就是使用virtualenv来安装。")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("$ sudo pip install virtualenv\n$ virtualenv scrapyenv\n$ cd scrapyenv\n$ source bin/activate\n$ pip install Scrapy\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br")])]),a("p",[s._v("安装后，只要在命令终端输入 scrapy，提示类似以下结果，代表已经安装成功。")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://www.runoob.com/wp-content/uploads/2018/10/3748346984-5a79c47d84aba_articlex.png",alt:""}})]),s._v(" "),a("hr"),s._v(" "),a("h2",{attrs:{id:"入门案例"}},[s._v("入门案例")]),s._v(" "),a("h3",{attrs:{id:"学习目标"}},[s._v("学习目标")]),s._v(" "),a("ul",[a("li",[s._v("创建一个Scrapy项目")]),s._v(" "),a("li",[s._v("定义提取的结构化数据(Item)")]),s._v(" "),a("li",[s._v("编写爬取网站的 Spider 并提取出结构化数据(Item)")]),s._v(" "),a("li",[s._v("编写 Item Pipelines 来存储提取到的Item(即结构化数据)")])]),s._v(" "),a("h3",{attrs:{id:"一-新建项目-scrapy-startproject"}},[s._v("一. 新建项目(scrapy startproject)")]),s._v(" "),a("p",[s._v("在开始爬取之前，必须创建一个新的Scrapy项目。进入自定义的项目目录中，运行下列命令：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("scrapy startproject mySpider\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("其中， mySpider 为项目名称，可以看到将会创建一个 mySpider 文件夹，目录结构大致如下：")]),s._v(" "),a("p",[s._v("下面来简单介绍一下各个主要文件的作用：")]),s._v(" "),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[s._v("mySpider/\n    scrapy.cfg\n    mySpider/\n        __init__.py\n        items.py\n        pipelines.py\n        settings.py\n        spiders/\n            __init__.py\n            "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("..")]),s._v(".\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br")])]),a("p",[s._v("这些文件分别是:")]),s._v(" "),a("ul",[a("li",[s._v("scrapy.cfg: 项目的配置文件。")]),s._v(" "),a("li",[s._v("mySpider/: 项目的Python模块，将会从这里引用代码。")]),s._v(" "),a("li",[s._v("mySpider/items.py: 项目的目标文件。")]),s._v(" "),a("li",[s._v("mySpider/pipelines.py: 项目的管道文件。")]),s._v(" "),a("li",[s._v("mySpider/settings.py: 项目的设置文件。")]),s._v(" "),a("li",[s._v("mySpider/spiders/: 存储爬虫代码目录。")])]),s._v(" "),a("h3",{attrs:{id:"二、明确目标-myspider-items-py"}},[s._v("二、明确目标(mySpider/items.py)")]),s._v(" "),a("p",[s._v("我们打算抓取 "),a("strong",[s._v("http://www.itcast.cn/channel/teacher.shtml")]),s._v(" 网站里的所有讲师的姓名、职称和个人信息。")]),s._v(" "),a("ol",[a("li",[a("p",[s._v("打开 mySpider 目录下的 items.py。")])]),s._v(" "),a("li",[a("p",[s._v("Item 定义结构化数据字段，用来保存爬取到的数据，有点像 Python 中的 dict，但是提供了一些额外的保护减少错误。")])]),s._v(" "),a("li",[a("p",[s._v("可以通过创建一个 scrapy.Item 类， 并且定义类型为 scrapy.Field 的类属性来定义一个 Item（可以理解成类似于 ORM 的映射关系）。")])])]),s._v(" "),a("p",[s._v("接下来，创建一个 ItcastItem 类，和构建 item 模型（model）。")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("import scrapy\n\nclass ItcastItem(scrapy.Item):\n   name = scrapy.Field()\n   title = scrapy.Field()\n   info = scrapy.Field()\n\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br")])]),a("h3",{attrs:{id:"三、制作爬虫-（spiders-itcastspider-py）"}},[s._v("三、制作爬虫 （spiders/itcastSpider.py）")]),s._v(" "),a("p",[s._v("爬虫功能要分两步：")]),s._v(" "),a("p",[a("strong",[s._v("1. 爬数据")])]),s._v(" "),a("p",[s._v("在当前目录下输入命令，将在mySpider/spider目录下创建一个名为itcast的爬虫，并指定爬取域的范围：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v('scrapy genspider itcast "itcast.cn"\n')])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("打开 mySpider/spider目录里的 itcast.py，默认增加了下列代码:")]),s._v(" "),a("div",{staticClass:"language-py line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" scrapy\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("class")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("ItcastSpider")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Spider"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    name "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"itcast"')]),s._v("\n    allowed_domains "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"itcast.cn"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n    start_urls "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'http://www.itcast.cn/'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("parse")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("pass")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br")])]),a("p",[s._v("其实也可以由我们自行创建itcast.py并编写上面的代码，只不过使用命令可以免去编写固定代码的麻烦")]),s._v(" "),a("p",[s._v("要建立一个Spider， 你必须用scrapy.Spider类创建一个子类，并确定了三个强制的属性 和 一个方法。")]),s._v(" "),a("p",[s._v('name = "" ：这个爬虫的识别名称，必须是唯一的，在不同的爬虫必须定义不同的名字。')]),s._v(" "),a("p",[s._v("allow_domains = [] 是搜索的域名范围，也就是爬虫的约束区域，规定爬虫只爬取这个域名下的网页，不存在的URL会被忽略。")]),s._v(" "),a("p",[s._v("start_urls = () ：爬取的URL元祖/列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些urls开始。其他子URL将会从这些起始URL中继承性生成。")]),s._v(" "),a("p",[s._v("parse(self, response) ：解析的方法，每个初始URL完成下载后将被调用，调用的时候传入从每一个URL传回的Response对象来作为唯一参数，主要作用如下：")]),s._v(" "),a("p",[s._v("负责解析返回的网页数据(response.body)，提取结构化数据(生成item)"),a("br"),s._v("\n生成需要下一页的URL请求。"),a("br"),s._v("\n将start_urls的值修改为需要爬取的第一个url")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v('start_urls = ("http://www.itcast.cn/channel/teacher.shtml",)\n')])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("修改parse()方法")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("def parse(self, response):\n    filename = \"teacher.html\"\n    open(filename, 'w').write(response.body)\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[s._v("然后运行一下看看，在mySpider目录下执行：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("scrapy crawl itcast\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("是的，就是 itcast，看上面代码，它是 ItcastSpider 类的 name 属性，也就是使用 scrapy genspider命令的唯一爬虫名。")]),s._v(" "),a("p",[s._v("运行之后，如果打印的日志出现 [scrapy] INFO: Spider closed (finished)，代表执行完成。 之后当前文件夹中就出现了一个 teacher.html 文件，里面就是我们刚刚要爬取的网页的全部源代码信息。")]),s._v(" "),a("p",[a("strong",[s._v("注意:")]),s._v(" Python2.x默认编码环境是ASCII，当和取回的数据编码格式不一致时，可能会造成乱码；我们可以指定保存内容的编码格式，一般情况下，我们可以在代码最上方添加")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v('import sys\nreload(sys)\nsys.setdefaultencoding("utf-8")\n')])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[s._v("这三行代码是 Python2.x 里解决中文编码的万能钥匙，经过这么多年的吐槽后 Python3 学乖了，默认编码是Unicode了...(祝大家早日拥抱Python3)")]),s._v(" "),a("p",[a("strong",[s._v("2. 取数据")])]),s._v(" "),a("p",[s._v("爬取整个网页完毕，接下来的就是的取过程了，首先观察页面源码：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v('<div class="li_txt">\n    <h3>  xxx  </h3>\n    <h4> xxxxx </h4>\n    <p> xxxxxxxx </p>\n')])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br")])]),a("p",[s._v("是不是一目了然？直接上 XPath 开始提取数据吧。")]),s._v(" "),a("p",[s._v("xpath 方法，我们只需要输入的 xpath 规则就可以定位到相应 html 标签节点，详细内容可以查看 "),a("a",{attrs:{href:"https://www.runoob.com/xpath/xpath-tutorial.html",target:"_blank",rel:"noopener noreferrer"}},[s._v("xpath 教程"),a("OutboundLink")],1),s._v("。")]),s._v(" "),a("p",[s._v("不会 xpath 语法没关系，Chrome 给我们提供了一键获取 xpath 地址的方法（"),a("strong",[s._v("右键->检查->copy->copy xpath")]),s._v("）,如下图:")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://www.runoob.com/wp-content/uploads/2018/10/xpath-chrome.jpg",alt:""}})]),s._v(" "),a("p",[s._v("这里给出一些 XPath 表达式的例子及对应的含义:")]),s._v(" "),a("ul",[a("li",[a("code",[s._v("/html/head/title")]),s._v(": 选择HTML文档中 "),a("code",[s._v("<head>")]),s._v(" 标签内的 "),a("code",[s._v("<title>")]),s._v(" 元素")]),s._v(" "),a("li",[a("code",[s._v("/html/head/title/text()")]),s._v(": 选择上面提到的 "),a("code",[s._v("<title>")]),s._v(" 元素的文字")]),s._v(" "),a("li",[a("code",[s._v("//td")]),s._v(": 选择所有的 "),a("code",[s._v("<td>")]),s._v(" 元素")]),s._v(" "),a("li",[a("code",[s._v('//div[@class="mine"]')]),s._v(": 选择所有具有 "),a("code",[s._v('class="mine"')]),s._v(" 属性的 "),a("code",[s._v("div")]),s._v(" 元素")])]),s._v(" "),a("p",[s._v("举例我们读取网站 "),a("strong",[s._v("http://www.itcast.cn/")]),s._v(" 的网站标题，修改 itcast.py 文件代码如下：：")]),s._v(" "),a("div",{staticClass:"language-py line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# -*- coding: utf-8 -*-")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" scrapy\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 以下三行是在 Python2.x版本中解决乱码问题，Python3.x 版本的可以去掉")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" sys\n"),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("reload")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("sys"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nsys"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("setdefaultencoding"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"utf-8"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("class")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("Opp2Spider")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Spider"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    name "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'itcast'")]),s._v("\n    allowed_domains "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'itcast.com'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n    start_urls "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'http://www.itcast.cn/'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("parse")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 获取网站标题")]),s._v("\n        context "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'/html/head/title/text()'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("   \n       \n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 提取网站标题")]),s._v("\n        title "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" context"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("extract_first"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("  \n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("title"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" \n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("pass")]),s._v("\n\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br")])]),a("p",[s._v("执行以下命令：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("$ scrapy crawl itcast\n...\n...\n传智播客官网-好口碑IT培训机构,一样的教育,不一样的品质\n...\n...\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br")])]),a("p",[s._v("我们之前在 mySpider/items.py 里定义了一个 ItcastItem 类。 这里引入进来:")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("from mySpider.items import ItcastItem\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("然后将我们得到的数据封装到一个 ItcastItem 对象中，可以保存每个老师的属性：")]),s._v(" "),a("div",{staticClass:"language-py line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" mySpider"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("items "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" ItcastItem\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("parse")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v('#open("teacher.html","wb").write(response.body).close()')]),s._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 存放老师信息的集合")]),s._v("\n    items "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" each "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("\"//div[@class='li_txt']\"")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 将我们得到的数据封装到一个 \\`ItcastItem\\` 对象")]),s._v("\n        item "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" ItcastItem"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#extract()方法返回的都是unicode字符串")]),s._v("\n        name "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" each"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"h3/text()"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("extract"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        title "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" each"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"h4/text()"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("extract"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        info "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" each"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"p/text()"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("extract"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#xpath返回的是包含一个元素的列表")]),s._v("\n        item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'name'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n        item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'title'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" title"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n        item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'info'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" info"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n\n        items"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 直接返回最后数据")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" items\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br")])]),a("p",[s._v("我们暂时先不处理管道，后面会详细介绍。")]),s._v(" "),a("h3",{attrs:{id:"保存数据"}},[s._v("保存数据")]),s._v(" "),a("p",[s._v("scrapy保存信息的最简单的方法主要有四种，-o 输出指定格式的文件，命令如下：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("scrapy crawl itcast -o teachers.json\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("json lines格式，默认为Unicode编码")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("scrapy crawl itcast -o teachers.jsonl\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("csv 逗号表达式，可用Excel打开")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("scrapy crawl itcast -o teachers.csv\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("xml格式")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("scrapy crawl itcast -o teachers.xml\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("h3",{attrs:{id:"思考"}},[s._v("思考")]),s._v(" "),a("p",[s._v("如果将代码改成下面形式，结果完全一样。")]),s._v(" "),a("p",[s._v("请思考 yield 在这里的作用("),a("a",{attrs:{href:"https://www.runoob.com/w3cnote/python-yield-used-analysis.html",target:"_blank",rel:"noopener noreferrer"}},[s._v("Python yield 使用浅析"),a("OutboundLink")],1),s._v(")：")]),s._v(" "),a("div",{staticClass:"language-py line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# -*- coding: utf-8 -*-")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" scrapy\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" mySpider"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("items "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" ItcastItem\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 以下三行是在 Python2.x版本中解决乱码问题，Python3.x 版本的可以去掉")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" sys\n"),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("reload")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("sys"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nsys"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("setdefaultencoding"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"utf-8"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("class")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("Opp2Spider")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Spider"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    name "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'itcast'")]),s._v("\n    allowed_domains "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'itcast.com'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n    start_urls "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"http://www.itcast.cn/channel/teacher.shtml"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("parse")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v('#open("teacher.html","wb").write(response.body).close()')]),s._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 存放老师信息的集合")]),s._v("\n        items "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" each "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("\"//div[@class='li_txt']\"")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 将我们得到的数据封装到一个 \\`ItcastItem\\` 对象")]),s._v("\n            item "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" ItcastItem"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#extract()方法返回的都是unicode字符串")]),s._v("\n            name "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" each"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"h3/text()"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("extract"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n            title "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" each"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"h4/text()"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("extract"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n            info "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" each"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"p/text()"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("extract"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#xpath返回的是包含一个元素的列表")]),s._v("\n            item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'name'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n            item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'title'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" title"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n            item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'info'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" info"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n\n            items"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 直接返回最后数据")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" items\n\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br"),a("span",{staticClass:"line-number"},[s._v("30")]),a("br"),a("span",{staticClass:"line-number"},[s._v("31")]),a("br"),a("span",{staticClass:"line-number"},[s._v("32")]),a("br"),a("span",{staticClass:"line-number"},[s._v("33")]),a("br"),a("span",{staticClass:"line-number"},[s._v("34")]),a("br"),a("span",{staticClass:"line-number"},[s._v("35")]),a("br"),a("span",{staticClass:"line-number"},[s._v("36")]),a("br"),a("span",{staticClass:"line-number"},[s._v("37")]),a("br"),a("span",{staticClass:"line-number"},[s._v("38")]),a("br")])]),a("blockquote",[a("p",[s._v("原文链接：https://segmentfault.com/a/1190000013178839")])])])}),[],!1,null,null,null);t.default=e.exports}}]);