(window.webpackJsonp=window.webpackJsonp||[]).push([[71],{463:function(s,t,a){"use strict";a.r(t);var e=a(25),n=Object(e.a)({},(function(){var s=this,t=s.$createElement,a=s._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[a("p"),a("div",{staticClass:"table-of-contents"},[a("ul",[a("li",[a("a",{attrs:{href:"#一、crawlspider"}},[s._v("一、CrawlSpider")])]),a("li",[a("a",{attrs:{href:"#二、item-loader"}},[s._v("二、Item Loader")])]),a("li",[a("a",{attrs:{href:"#_1-identity"}},[s._v("1\\. Identity")])]),a("li",[a("a",{attrs:{href:"#_2-takefirst"}},[s._v("2\\. TakeFirst")])]),a("li",[a("a",{attrs:{href:"#_3-join"}},[s._v("3\\. Join")])]),a("li",[a("a",{attrs:{href:"#_4-compose"}},[s._v("4\\. Compose")])]),a("li",[a("a",{attrs:{href:"#_5-mapcompose"}},[s._v("5\\. MapCompose")])]),a("li",[a("a",{attrs:{href:"#_6-selectjmes"}},[s._v("6\\. SelectJmes")])]),a("li",[a("a",{attrs:{href:"#三、本节目标"}},[s._v("三、本节目标")])]),a("li",[a("a",{attrs:{href:"#四、新建项目"}},[s._v("四、新建项目")])]),a("li",[a("a",{attrs:{href:"#五、定义rule"}},[s._v("五、定义Rule")])]),a("li",[a("a",{attrs:{href:"#六、解析页面"}},[s._v("六、解析页面")])]),a("li",[a("a",{attrs:{href:"#七、通用配置抽取"}},[s._v("七、通用配置抽取")])]),a("li",[a("a",{attrs:{href:"#八、本节代码"}},[s._v("八、本节代码")])]),a("li",[a("a",{attrs:{href:"#九、结语"}},[s._v("九、结语")])])])]),s._v("\n通过Scrapy，我们可以轻松地完成一个站点爬虫的编写。但如果抓取的站点量非常大，比如爬取各大媒体的新闻信息，多个Spider则可能包含很多重复代码。"),a("p"),s._v(" "),a("p",[s._v("如果我们将各个站点的Spider的公共部分保留下来，不同的部分提取出来作为单独的配置，如爬取规则、页面解析方式等抽离出来做成一个配置文件，那么我们在新增一个爬虫的时候，只需要实现这些网站的爬取规则和提取规则即可。")]),s._v(" "),a("p",[s._v("本节我们就来探究一下Scrapy通用爬虫的实现方法。")]),s._v(" "),a("h2",{attrs:{id:"一、crawlspider"}},[s._v("一、CrawlSpider")]),s._v(" "),a("p",[s._v("在实现通用爬虫之前，我们需要先了解一下CrawlSpider，其官方文档链接为：http://scrapy.readthedocs.io/en/latest/topics/spiders.html#crawlspider。")]),s._v(" "),a("p",[s._v("CrawlSpider是Scrapy提供的一个通用Spider。在Spider里，我们可以指定一些爬取规则来实现页面的提取，这些爬取规则由一个专门的数据结构Rule表示。Rule里包含提取和跟进页面的配置，Spider会根据Rule来确定当前页面中的哪些链接需要继续爬取、哪些页面的爬取结果需要用哪个方法解析等。")]),s._v(" "),a("p",[a("code",[s._v("CrawlSpider")]),s._v("继承自"),a("code",[s._v("Spider")]),s._v("类。除了"),a("code",[s._v("Spider")]),s._v("类的所有方法和属性，它还提供了一个非常重要的属性和方法。")]),s._v(" "),a("ul",[a("li",[a("p",[a("code",[s._v("rules")]),s._v("，它是爬取规则属性，是包含一个或多个"),a("code",[s._v("Rule")]),s._v("对象的列表。每个"),a("code",[s._v("Rule")]),s._v("对爬取网站的动作都做了定义，CrawlSpider会读取"),a("code",[s._v("rules")]),s._v("的每一个"),a("code",[s._v("Rule")]),s._v("并进行解析。")])]),s._v(" "),a("li",[a("p",[a("code",[s._v("parse_start_url()")]),s._v("，它是一个可重写的方法。当"),a("code",[s._v("start_urls")]),s._v("里对应的Request得到Response时，该方法被调用，它会分析Response并必须返回"),a("code",[s._v("Item")]),s._v("对象或者"),a("code",[s._v("Request")]),s._v("对象。")])])]),s._v(" "),a("p",[s._v("这里最重要的内容莫过于"),a("code",[s._v("Rule")]),s._v("的定义了，它的定义和参数如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("class scrapy.contrib.spiders.Rule(link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None)\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("下面将依次说明"),a("code",[s._v("Rule")]),s._v("的参数。")]),s._v(" "),a("ul",[a("li",[a("p",[a("strong",[a("code",[s._v("link_extractor")])]),s._v("：是Link Extractor对象。通过它，Spider可以知道从爬取的页面中提取哪些链接。提取出的链接会自动生成Request。它又是一个数据结构，一般常用"),a("code",[s._v("LxmlLinkExtractor")]),s._v("对象作为参数，其定义和参数如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("class scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor(allow=(), deny=(), allow_domains=(), deny_domains=(), deny_extensions=None, restrict_xpaths=(), restrict_css=(), tags=('a', 'area'), attrs=('href', ), canonicalize=False, unique=True, process_value=None, strip=True)\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[a("code",[s._v("allow")]),s._v("是一个正则表达式或正则表达式列表，它定义了从当前页面提取出的链接哪些是符合要求的，只有符合要求的链接才会被跟进。"),a("code",[s._v("deny")]),s._v("则相反。"),a("code",[s._v("allow_domains")]),s._v("定义了符合要求的域名，只有此域名的链接才会被跟进生成新的Request，它相当于域名白名单。"),a("code",[s._v("deny_domains")]),s._v("则相反，相当于域名黑名单。"),a("code",[s._v("restrict_xpaths")]),s._v("定义了从当前页面中XPath匹配的区域提取链接，其值是XPath表达式或XPath表达式列表。"),a("code",[s._v("restrict_css")]),s._v("定义了从当前页面中CSS选择器匹配的区域提取链接，其值是CSS选择器或CSS选择器列表。还有一些其他参数代表了提取链接的标签、是否去重、链接的处理等内容，使用的频率不高。可以参考文档的参数说明：http://scrapy.readthedocs.io/en/latest/topics/link-extractors.html#module-scrapy.linkextractors.lxmlhtml。")])]),s._v(" "),a("li",[a("p",[a("strong",[a("code",[s._v("callback")])]),s._v("：即回调函数，和之前定义Request的"),a("code",[s._v("callback")]),s._v("有相同的意义。每次从"),a("code",[s._v("link_extractor")]),s._v("中获取到链接时，该函数将会调用。该回调函数接收一个"),a("code",[s._v("response")]),s._v("作为其第一个参数，并返回一个包含Item或Request对象的列表。注意，避免使用"),a("code",[s._v("parse()")]),s._v("作为回调函数。由于"),a("code",[s._v("CrawlSpider")]),s._v("使用"),a("code",[s._v("parse()")]),s._v("方法来实现其逻辑，如果"),a("code",[s._v("parse()")]),s._v("方法覆盖了，"),a("code",[s._v("CrawlSpider")]),s._v("将会运行失败。")])]),s._v(" "),a("li",[a("p",[a("strong",[a("code",[s._v("cb_kwargs")])]),s._v("：字典，它包含传递给回调函数的参数。")])]),s._v(" "),a("li",[a("p",[a("strong",[a("code",[s._v("follow")])]),s._v("：布尔值，即"),a("code",[s._v("True")]),s._v("或"),a("code",[s._v("False")]),s._v("，它指定根据该规则从"),a("code",[s._v("response")]),s._v("提取的链接是否需要跟进。如果"),a("code",[s._v("callback")]),s._v("参数为"),a("code",[s._v("None")]),s._v("，"),a("code",[s._v("follow")]),s._v("默认设置为"),a("code",[s._v("True")]),s._v("，否则默认为"),a("code",[s._v("False")]),s._v("。")])]),s._v(" "),a("li",[a("p",[a("strong",[a("code",[s._v("process_links")])]),s._v("：指定处理函数，从"),a("code",[s._v("link_extractor")]),s._v("中获取到链接列表时，该函数将会调用，它主要用于过滤。")])]),s._v(" "),a("li",[a("p",[a("strong",[a("code",[s._v("process_request")])]),s._v("：同样是指定处理函数，根据该Rule提取到每个Request时，该函数都会调用，对Request进行处理。该函数必须返回"),a("code",[s._v("Request")]),s._v("或者"),a("code",[s._v("None")]),s._v("。")])])]),s._v(" "),a("p",[s._v("以上内容便是CrawlSpider中的核心Rule的基本用法。但这些内容可能还不足以完成一个CrawlSpider爬虫。下面我们利用CrawlSpider实现新闻网站的爬取实例，来更好地理解Rule的用法。")]),s._v(" "),a("h2",{attrs:{id:"二、item-loader"}},[s._v("二、Item Loader")]),s._v(" "),a("p",[s._v("我们了解了利用CrawlSpider的Rule来定义页面的爬取逻辑，这是可配置化的一部分内容。但是，Rule并没有对Item的提取方式做规则定义。对于Item的提取，我们需要借助另一个模块Item Loader来实现。")]),s._v(" "),a("p",[s._v("Item Loader提供一种便捷的机制来帮助我们方便地提取Item。它提供的一系列API可以分析原始数据对Item进行赋值。Item提供的是保存抓取数据的容器，而Item Loader提供的是填充容器的机制。有了它，数据的提取会变得更加规则化。")]),s._v(" "),a("p",[s._v("Item Loader的API如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("class scrapy.loader.ItemLoader([item, selector, response, ] **kwargs)\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("Item Loader的API返回一个新的Item Loader来填充给定的Item。如果没有给出Item，则使用中的类自动实例化"),a("code",[s._v("default_item_class")]),s._v("。另外，它传入"),a("code",[s._v("selector")]),s._v("和"),a("code",[s._v("response")]),s._v("参数来使用选择器或响应参数实例化。")]),s._v(" "),a("p",[s._v("下面将依次说明Item Loader的API参数。")]),s._v(" "),a("ul",[a("li",[a("p",[a("strong",[a("code",[s._v("item")])]),s._v("：它是"),a("code",[s._v("Item")]),s._v("对象，可以调用"),a("code",[s._v("add_xpath()")]),s._v("、"),a("code",[s._v("add_css()")]),s._v("或"),a("code",[s._v("add_value()")]),s._v("等方法来填充"),a("code",[s._v("Item")]),s._v("对象。")])]),s._v(" "),a("li",[a("p",[a("strong",[a("code",[s._v("selector")])]),s._v("：它是"),a("code",[s._v("Selector")]),s._v("对象，用来提取填充数据的选择器。")])]),s._v(" "),a("li",[a("p",[a("strong",[a("code",[s._v("response")])]),s._v("：它是"),a("code",[s._v("Response")]),s._v("对象，用于使用构造选择器的Response。")])])]),s._v(" "),a("p",[s._v("一个比较典型的Item Loader实例如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("from scrapy.loader import ItemLoader\nfrom project.items import Product\n\ndef parse(self, response):\n    loader = ItemLoader(item=Product(), response=response)\n    loader.add_xpath('name', '//div[@class=\"product_name\"]')\n    loader.add_xpath('name', '//div[@class=\"product_title\"]')\n    loader.add_xpath('price', '//p[@id=\"price\"]')\n    loader.add_css('stock', 'p#stock]')\n    loader.add_value('last_updated', 'today')\n    return loader.load_item()\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br")])]),a("p",[s._v("这里首先声明一个Product Item，用该"),a("code",[s._v("Item")]),s._v("和"),a("code",[s._v("Response")]),s._v("对象实例化"),a("code",[s._v("ItemLoader")]),s._v("，调用"),a("code",[s._v("add_xpath()")]),s._v("方法把来自两个不同位置的数据提取出来，分配给"),a("code",[s._v("name")]),s._v("属性，再用"),a("code",[s._v("add_xpath()")]),s._v("、"),a("code",[s._v("add_css()")]),s._v("、"),a("code",[s._v("add_value()")]),s._v("等方法对不同属性依次赋值，最后调用"),a("code",[s._v("load_item()")]),s._v("方法实现Item的解析。这种方式比较规则化，我们可以把一些参数和规则单独提取出来做成配置文件或存到数据库，即可实现可配置化。")]),s._v(" "),a("p",[s._v("另外，Item Loader每个字段中都包含了一个Input Processor（输入处理器）和一个Output Processor（输出处理器）。Input Processor收到数据时立刻提取数据，Input Processor的结果被收集起来并且保存在ItemLoader内，但是不分配给Item。收集到所有的数据后，"),a("code",[s._v("load_item()")]),s._v("方法被调用来填充再生成"),a("code",[s._v("Item")]),s._v("对象。在调用时会先调用Output Processor来处理之前收集到的数据，然后再存入Item中，这样就生成了Item。")]),s._v(" "),a("p",[s._v("下面将介绍一些内置的的Processor。")]),s._v(" "),a("h2",{attrs:{id:"_1-identity"}},[s._v("1. Identity")]),s._v(" "),a("p",[a("code",[s._v("Identity")]),s._v("是最简单的Processor，不进行任何处理，直接返回原来的数据。")]),s._v(" "),a("h2",{attrs:{id:"_2-takefirst"}},[s._v("2. TakeFirst")]),s._v(" "),a("p",[a("code",[s._v("TakeFirst")]),s._v("返回列表的第一个非空值，类似"),a("code",[s._v("extract_first()")]),s._v("的功能，常用作Output Processor，如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("from scrapy.loader.processors import TakeFirst\nprocessor = TakeFirst()\nprint(processor(['', 1, 2, 3]))\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[s._v("输出结果如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("1\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("经过此Processor处理后的结果返回了第一个不为空的值。")]),s._v(" "),a("h2",{attrs:{id:"_3-join"}},[s._v("3. Join")]),s._v(" "),a("p",[a("code",[s._v("Join")]),s._v("方法相当于字符串的"),a("code",[s._v("join()")]),s._v("方法，可以把列表拼合成字符串，字符串默认使用空格分隔，如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("from scrapy.loader.processors import Join\nprocessor = Join()\nprint(processor(['one', 'two', 'three']))\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[s._v("输出结果如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("one two three\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("它也可以通过参数更改默认的分隔符，例如改成逗号：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("from scrapy.loader.processors import Join\nprocessor = Join(',')\nprint(processor(['one', 'two', 'three']))\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[s._v("运行结果如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("one,two,three\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("h2",{attrs:{id:"_4-compose"}},[s._v("4. Compose")]),s._v(" "),a("p",[a("code",[s._v("Compose")]),s._v("是用给定的多个函数的组合而构造的Processor，每个输入值被传递到第一个函数，其输出再传递到第二个函数，依次类推，直到最后一个函数返回整个处理器的输出，如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("from scrapy.loader.processors import Compose\nprocessor = Compose(str.upper, lambda s: s.strip())\nprint(processor(' hello world'))\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[s._v("运行结果如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("HELLO WORLD\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("在这里我们构造了一个Compose Processor，传入一个开头带有空格的字符串。Compose Processor的参数有两个：第一个是"),a("code",[s._v("str.upper")]),s._v("，它可以将字母全部转为大写；第二个是一个匿名函数，它调用"),a("code",[s._v("strip()")]),s._v("方法去除头尾空白字符。"),a("code",[s._v("Compose")]),s._v("会顺次调用两个参数，最后返回结果的字符串全部转化为大写并且去除了开头的空格。")]),s._v(" "),a("h2",{attrs:{id:"_5-mapcompose"}},[s._v("5. MapCompose")]),s._v(" "),a("p",[s._v("与"),a("code",[s._v("Compose")]),s._v("类似，"),a("code",[s._v("MapCompose")]),s._v("可以迭代处理一个列表输入值，如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("from scrapy.loader.processors import MapCompose\nprocessor = MapCompose(str.upper, lambda s: s.strip())\nprint(processor(['Hello', 'World', 'Python']))\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[s._v("运行结果如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("['HELLO', 'WORLD', 'PYTHON']\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("被处理的内容是一个可迭代对象，"),a("code",[s._v("MapCompose")]),s._v("会将该对象遍历然后依次处理。")]),s._v(" "),a("h2",{attrs:{id:"_6-selectjmes"}},[s._v("6. SelectJmes")]),s._v(" "),a("p",[a("code",[s._v("SelectJmes")]),s._v("可以查询JSON，传入Key，返回查询所得的Value。不过需要先安装Jmespath库才可以使用它，命令如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("pip3 install jmespath\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("安装好Jmespath之后，便可以使用这个Processor了，如下所示：")]),s._v(" "),a("div",{staticClass:"language-py line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("loader"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("processors "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" SelectJmes\nproc "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" SelectJmes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'foo'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nprocessor "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" SelectJmes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'foo'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("processor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'foo'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'bar'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br")])]),a("p",[s._v("运行结果如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("bar\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("以上内容便是一些常用的Processor，在本节的实例中我们会使用Processor来进行数据的处理。")]),s._v(" "),a("p",[s._v("接下来，我们用一个实例来了解Item Loader的用法。")]),s._v(" "),a("h2",{attrs:{id:"三、本节目标"}},[s._v("三、本节目标")]),s._v(" "),a("p",[s._v("我们以中华网科技类新闻为例，来了解CrawlSpider和Item Loader的用法，再提取其可配置信息实现可配置化。官网链接为：http://tech.china.com/。我们需要爬取它的科技类新闻内容，链接为：http://tech.china.com/articles/，页面如下图所示。")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/5/21/163817bc897130b9?imageView2/0/w/1280/h/960/format/webp/ignore-error/1",alt:""}})]),s._v(" "),a("p",[s._v("我们要抓取新闻列表中的所有分页的新闻详情，包括标题、正文、时间、来源等信息。")]),s._v(" "),a("h2",{attrs:{id:"四、新建项目"}},[s._v("四、新建项目")]),s._v(" "),a("p",[s._v("首先新建一个Scrapy项目，名为scrapyuniversal，如下所示：")]),s._v(" "),a("div",{staticClass:"language-py line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[s._v("scrapy startproject scrapyuniversal\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("创建一个CrawlSpider，需要先制定一个模板。我们可以先看看有哪些可用模板，命令如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("scrapy genspider -l\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("运行结果如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("Available templates:\n  basic\n  crawl\n  csvfeed\n  xmlfeed\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br")])]),a("p",[s._v("之前创建Spider的时候，我们默认使用了第一个模板"),a("code",[s._v("basic")]),s._v("。这次要创建CrawlSpider，就需要使用第二个模板"),a("code",[s._v("crawl")]),s._v("，创建命令如下所示：")]),s._v(" "),a("div",{staticClass:"language-py line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[s._v("scrapy genspider "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),s._v("t crawl china tech"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("china"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("com\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("运行之后便会生成一个CrawlSpider，其内容如下所示：")]),s._v(" "),a("div",{staticClass:"language-py line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("linkextractors "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" LinkExtractor\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("spiders "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" CrawlSpider"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" Rule\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("class")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("ChinaSpider")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("CrawlSpider"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    name "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'china'")]),s._v("\n    allowed_domains "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'tech.china.com'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n    start_urls "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'http://tech.china.com/'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n\n    rules "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n        Rule"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("LinkExtractor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("allow"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("r'Items/'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" callback"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'parse_item'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" follow"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("parse_item")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        i "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#i['domain_id'] = response.xpath('//input[@id=\"sid\"]/@value').extract()")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#i['name'] = response.xpath('//div[@id=\"name\"]').extract()")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#i['description'] = response.xpath('//div[@id=\"description\"]').extract()")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" i\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br")])]),a("p",[s._v("这次生成的Spider内容多了一个"),a("code",[s._v("rules")]),s._v("属性的定义。Rule的第一个参数是"),a("code",[s._v("LinkExtractor")]),s._v("，就是上文所说的"),a("code",[s._v("LxmlLinkExtractor")]),s._v("，只是名称不同。同时，默认的回调函数也不再是"),a("code",[s._v("parse")]),s._v("，而是"),a("code",[s._v("parse_item")]),s._v("。")]),s._v(" "),a("h2",{attrs:{id:"五、定义rule"}},[s._v("五、定义Rule")]),s._v(" "),a("p",[s._v("要实现新闻的爬取，我们需要做的就是定义好Rule，然后实现解析函数。下面我们就来一步步实现这个过程。")]),s._v(" "),a("p",[s._v("首先将"),a("code",[s._v("start_urls")]),s._v("修改为起始链接，代码如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("start_urls = ['http://tech.china.com/articles/']\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("之后，Spider爬取"),a("code",[s._v("start_urls")]),s._v("里面的每一个链接。所以这里第一个爬取的页面就是我们刚才所定义的链接。得到Response之后，Spider就会根据每一个Rule来提取这个页面内的超链接，去生成进一步的Request。接下来，我们就需要定义Rule来指定提取哪些链接。")]),s._v(" "),a("p",[s._v("当前页面如下图所示。")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/5/21/163817bc899e9126?imageView2/0/w/1280/h/960/format/webp/ignore-error/1",alt:""}})]),s._v(" "),a("p",[s._v("这是新闻的列表页，下一步自然就是将列表中的每条新闻详情的链接提取出来。这里直接指定这些链接所在区域即可。查看源代码，所有链接都在ID为"),a("code",[s._v("left_side")]),s._v("的节点内，具体来说是它内部的"),a("code",[s._v("class")]),s._v("为"),a("code",[s._v("con_item")]),s._v("的节点，如下图所示。")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/5/21/163817bc8a482d32?imageView2/0/w/1280/h/960/format/webp/ignore-error/1",alt:""}})]),s._v(" "),a("p",[s._v("此处我们可以用"),a("code",[s._v("LinkExtractor")]),s._v("的"),a("code",[s._v("restrict_xpaths")]),s._v("属性来指定，之后Spider就会从这个区域提取所有的超链接并生成Request。但是，每篇文章的导航中可能还有一些其他的超链接标签，我们只想把需要的新闻链接提取出来。真正的新闻链接路径都是以"),a("code",[s._v("article")]),s._v("开头的，我们用一个正则表达式将其匹配出来再赋值给"),a("code",[s._v("allow")]),s._v("参数即可。另外，这些链接对应的页面其实就是对应的新闻详情页，而我们需要解析的就是新闻的详情信息，所以此处还需要指定一个回调函数"),a("code",[s._v("callback")]),s._v("。")]),s._v(" "),a("p",[s._v("到现在我们就可以构造出一个Rule了，代码如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("Rule(LinkExtractor(allow='article\\/.*\\.html', restrict_xpaths='//div[@id=\"left_side\"]//div[@class=\"con_item\"]'), callback='parse_item')\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("接下来，我们还要让当前页面实现分页功能，所以还需要提取下一页的链接。分析网页源码之后可以发现下一页链接是在ID为pageStyle的节点内，如下图所示。")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/5/21/163817bc89a22b6c?imageView2/0/w/1280/h/960/format/webp/ignore-error/1",alt:""}})]),s._v(" "),a("p",[s._v("但是，下一页节点和其他分页链接区分度不高，要取出此链接我们可以直接用XPath的文本匹配方式，所以这里我们直接用"),a("code",[s._v("LinkExtractor")]),s._v("的"),a("code",[s._v("restrict_xpaths")]),s._v("属性来指定提取的链接即可。另外，我们不需要像新闻详情页一样去提取此分页链接对应的页面详情信息，也就是不需要生成Item，所以不需要加"),a("code",[s._v("callback")]),s._v("参数。另外这下一页的页面如果请求成功了就需要继续像上述情况一样分析，所以它还需要加一个"),a("code",[s._v("follow")]),s._v("参数为"),a("code",[s._v("True")]),s._v("，代表继续跟进匹配分析。其实，"),a("code",[s._v("follow")]),s._v("参数也可以不加，因为当"),a("code",[s._v("callback")]),s._v("为空的时候，"),a("code",[s._v("follow")]),s._v("默认为"),a("code",[s._v("True")]),s._v("。此处Rule定义为如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v('Rule(LinkExtractor(restrict_xpaths=\'//div[@id="pageStyle"]//a[contains(., "下一页")]\'))\n')])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("所以现在"),a("code",[s._v("rules")]),s._v("就变成了：")]),s._v(" "),a("div",{staticClass:"language-py line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[s._v("rules "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n    Rule"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("LinkExtractor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("allow"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'article\\/.*\\.html'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" restrict_xpaths"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('\'//div[@id="left_side"]//div[@class="con_item"]\'')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" callback"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'parse_item'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    Rule"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("LinkExtractor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("restrict_xpaths"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('\'//div[@id="pageStyle"]//a[contains(., "下一页")]\'')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br")])]),a("p",[s._v("接着我们运行代码，命令如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("scrapy crawl china\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("现在已经实现页面的翻页和详情页的抓取了，我们仅仅通过定义了两个Rule即实现了这样的功能，运行效果如下图所示。")]),s._v(" "),a("p",[s._v('![](data:image/svg+xml;utf8,<?xml version="1.0"?>'),a("svg",{attrs:{xmlns:"http://www.w3.org/2000/svg",version:"1.1",width:"700",height:"465"}}),s._v(")")]),s._v(" "),a("h2",{attrs:{id:"六、解析页面"}},[s._v("六、解析页面")]),s._v(" "),a("p",[s._v("接下来我们需要做的就是解析页面内容了，将标题、发布时间、正文、来源提取出来即可。首先定义一个Item，如下所示：")]),s._v(" "),a("div",{staticClass:"language-py line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" scrapy "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" Field"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" Item\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("class")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("NewsItem")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("Item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    title "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" Field"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    url "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" Field"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    text "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" Field"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    datetime "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" Field"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    source "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" Field"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    website "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" Field"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br")])]),a("p",[s._v("这里的字段分别指新闻标题、链接、正文、发布时间、来源、站点名称，其中站点名称直接赋值为中华网。因为既然是通用爬虫，肯定还有很多爬虫也来爬取同样结构的其他站点的新闻内容，所以需要一个字段来区分一下站点名称。")]),s._v(" "),a("p",[s._v("详情页的预览图如下图所示。")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/5/21/163817bc89d3161d?imageView2/0/w/1280/h/960/format/webp/ignore-error/1",alt:""}})]),s._v(" "),a("p",[s._v("如果像之前一样提取内容，就直接调用"),a("code",[s._v("response")]),s._v("变量的"),a("code",[s._v("xpath()")]),s._v("、"),a("code",[s._v("css()")]),s._v("等方法即可。这里"),a("code",[s._v("parse_item()")]),s._v("方法的实现如下所示：")]),s._v(" "),a("div",{staticClass:"language-py line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("parse_item")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    item "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" NewsItem"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'title'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'//h1[@id=\"chan_newsTitle\"]/text()'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("extract_first"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'url'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("url\n    item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'text'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("''")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("join"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'//div[@id=\"chan_newsDetail\"]//text()'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("extract"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("strip"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'datetime'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'//div[@id=\"chan_newsInfo\"]/text()'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("re_first"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'(\\d+-\\d+-\\d+\\s\\d+:\\d+:\\d+)'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'source'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'//div[@id=\"chan_newsInfo\"]/text()'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("re_first"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'来源：(.*)'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("strip"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'website'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'中华网'")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("yield")]),s._v(" item\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br")])]),a("p",[s._v("这样我们就把每条新闻的信息提取形成了一个NewsItem对象。")]),s._v(" "),a("p",[s._v("这时实际上我们就已经完成了Item的提取。再运行一下Spider，如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("scrapy crawl china\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("输出内容如下图所示。")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/5/21/163817bca491c415?imageView2/0/w/1280/h/960/format/webp/ignore-error/1",alt:""}})]),s._v(" "),a("p",[s._v("现在我们就可以成功将每条新闻的信息提取出来。")]),s._v(" "),a("p",[s._v("不过我们发现这种提取方式非常不规整。下面我们再用"),a("code",[s._v("Item Loader")]),s._v("，通过"),a("code",[s._v("add_xpath()")]),s._v("、"),a("code",[s._v("add_css()")]),s._v("、"),a("code",[s._v("add_value()")]),s._v("等方式实现配置化提取。我们可以改写"),a("code",[s._v("parse_item()")]),s._v("，如下所示：")]),s._v(" "),a("div",{staticClass:"language-py line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("parse_item")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    loader "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" ChinaLoader"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("item"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("NewsItem"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" response"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    loader"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("add_xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'title'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'//h1[@id=\"chan_newsTitle\"]/text()'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    loader"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("add_value"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'url'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("url"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    loader"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("add_xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'text'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'//div[@id=\"chan_newsDetail\"]//text()'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    loader"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("add_xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'datetime'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'//div[@id=\"chan_newsInfo\"]/text()'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" re"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'(\\d+-\\d+-\\d+\\s\\d+:\\d+:\\d+)'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    loader"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("add_xpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'source'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'//div[@id=\"chan_newsInfo\"]/text()'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" re"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'来源：(.*)'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    loader"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("add_value"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'website'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'中华网'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("yield")]),s._v(" loader"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("load_item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br")])]),a("p",[s._v("这里我们定义了一个"),a("code",[s._v("ItemLoader")]),s._v("的子类，名为"),a("code",[s._v("ChinaLoader")]),s._v("，其实现如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("from scrapy.loader import ItemLoader\nfrom scrapy.loader.processors import TakeFirst, Join, Compose\n\nclass NewsLoader(ItemLoader):\n    default_output_processor = TakeFirst()\n\nclass ChinaLoader(NewsLoader):\n    text_out = Compose(Join(), lambda s: s.strip())\n    source_out = Compose(Join(), lambda s: s.strip())\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br")])]),a("p",[a("code",[s._v("ChinaLoader")]),s._v("继承了"),a("code",[s._v("NewsLoader")]),s._v("类，其内定义了一个通用的"),a("code",[s._v("Out Processor")]),s._v("为"),a("code",[s._v("TakeFirst")]),s._v("，这相当于之前所定义的"),a("code",[s._v("extract_first()")]),s._v("方法的功能。我们在"),a("code",[s._v("ChinaLoader")]),s._v("中定义了"),a("code",[s._v("text_out")]),s._v("和"),a("code",[s._v("source_out")]),s._v("字段。这里使用了一个Compose Processor，它有两个参数：第一个参数"),a("code",[s._v("Join")]),s._v("也是一个Processor，它可以把列表拼合成一个字符串；第二个参数是一个匿名函数，可以将字符串的头尾空白字符去掉。经过这一系列处理之后，我们就将列表形式的提取结果转化为去重头尾空白字符的字符串。")]),s._v(" "),a("p",[s._v("代码重新运行，提取效果是完全一样的。")]),s._v(" "),a("p",[s._v("至此，我们已经实现了爬虫的半通用化配置。")]),s._v(" "),a("h2",{attrs:{id:"七、通用配置抽取"}},[s._v("七、通用配置抽取")]),s._v(" "),a("p",[s._v("为什么现在只做到了半通用化？如果我们需要扩展其他站点，仍然需要创建一个新的CrawlSpider，定义这个站点的Rule，单独实现"),a("code",[s._v("parse_item()")]),s._v("方法。还有很多代码是重复的，如CrawlSpider的变量、方法名几乎都是一样的。那么我们可不可以把多个类似的几个爬虫的代码共用，把完全不相同的地方抽离出来，做成可配置文件呢？")]),s._v(" "),a("p",[s._v("当然可以。那我们可以抽离出哪些部分？所有的变量都可以抽取，如"),a("code",[s._v("name")]),s._v("、"),a("code",[s._v("allowed_domains")]),s._v("、"),a("code",[s._v("start_urls")]),s._v("、"),a("code",[s._v("rules")]),s._v("等。这些变量在CrawlSpider初始化的时候赋值即可。我们就可以新建一个通用的Spider来实现这个功能，命令如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("scrapy genspider -t crawl universal universal\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("这个全新的Spider名为"),a("code",[s._v("universal")]),s._v("。接下来，我们将刚才所写的Spider内的属性抽离出来配置成一个JSON，命名为china.json，放到configs文件夹内，和spiders文件夹并列，代码如下所示：")]),s._v(" "),a("div",{staticClass:"language-js line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-js"}},[a("code",[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"spider"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"universal"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"website"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"中华网科技"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"type"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"新闻"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"index"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"http://tech.china.com/"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"settings"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"USER_AGENT"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36"')]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"start_urls"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"http://tech.china.com/articles/"')]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"allowed_domains"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"tech.china.com"')]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"rules"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"china"')]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br")])]),a("p",[s._v("第一个字段"),a("code",[s._v("spider")]),s._v("即Spider的名称，在这里是"),a("code",[s._v("universal")]),s._v("。后面是站点的描述，比如站点名称、类型、首页等。随后的"),a("code",[s._v("settings")]),s._v("是该Spider特有的"),a("code",[s._v("settings")]),s._v("配置，如果要覆盖全局项目，settings.py内的配置可以单独为其配置。随后是Spider的一些属性，如"),a("code",[s._v("start_urls")]),s._v("、"),a("code",[s._v("allowed_domains")]),s._v("、"),a("code",[s._v("rules")]),s._v("等。"),a("code",[s._v("rules")]),s._v("也可以单独定义成一个rules.py文件，做成配置文件，实现Rule的分离，如下所示：")]),s._v(" "),a("div",{staticClass:"language-py line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("linkextractors "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" LinkExtractor\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("spiders "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" Rule\n\nrules "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'china'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n        Rule"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("LinkExtractor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("allow"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'article\\/.*\\.html'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" restrict_xpaths"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('\'//div[@id="left_side"]//div[@class="con_item"]\'')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n             callback"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'parse_item'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n        Rule"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("LinkExtractor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("restrict_xpaths"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('\'//div[@id="pageStyle"]//a[contains(., "下一页")]\'')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br")])]),a("p",[s._v("这样我们将基本的配置抽取出来。如果要启动爬虫，只需要从该配置文件中读取然后动态加载到Spider中即可。所以我们需要定义一个读取该JSON文件的方法，如下所示：")]),s._v(" "),a("div",{staticClass:"language-py line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" os"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("path "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" realpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" dirname\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" json\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("get_config")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    path "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" dirname"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("realpath"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("__file__"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'/configs/'")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),s._v(" name "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'.json'")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("with")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("open")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'r'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" encoding"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'utf-8'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" f"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" json"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("loads"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("f"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("read"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br")])]),a("p",[s._v("定义了"),a("code",[s._v("get_config()")]),s._v("方法之后，我们只需要向其传入JSON配置文件的名称即可获取此JSON配置信息。随后我们定义入口文件run.py，把它放在项目根目录下，它的作用是启动Spider，如下所示：")]),s._v(" "),a("div",{staticClass:"language-py line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" sys\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("utils"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("project "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" get_project_settings\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" scrapyuniversal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("spiders"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("universal "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" UniversalSpider\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" scrapyuniversal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("utils "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" get_config\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("crawler "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" CrawlerProcess\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("run")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    name "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" sys"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("argv"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n    custom_settings "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" get_config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 爬取使用的Spider名称")]),s._v("\n    spider "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" custom_settings"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("get"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'spider'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'universal'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    project_settings "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" get_project_settings"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    settings "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("dict")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("project_settings"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("copy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 合并配置")]),s._v("\n    settings"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("update"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("custom_settings"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("get"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'settings'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    process "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" CrawlerProcess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("settings"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 启动爬虫")]),s._v("\n    process"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("crawl"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("spider"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("**")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'name'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    process"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("start"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("if")]),s._v(" __name__ "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("==")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'__main__'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    run"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br")])]),a("p",[s._v("运行入口为"),a("code",[s._v("run()")]),s._v("。首先获取命令行的参数并赋值为"),a("code",[s._v("name")]),s._v("，"),a("code",[s._v("name")]),s._v("就是JSON文件的名称，其实就是要爬取的目标网站的名称。我们首先利用"),a("code",[s._v("get_config()")]),s._v("方法，传入该名称读取刚才定义的配置文件。获取爬取使用的"),a("code",[s._v("spider")]),s._v("的名称、配置文件中的"),a("code",[s._v("settings")]),s._v("配置，然后将获取到的"),a("code",[s._v("settings")]),s._v("配置和项目全局的"),a("code",[s._v("settings")]),s._v("配置做了合并。新建一个CrawlerProcess，传入爬取使用的配置。调用"),a("code",[s._v("crawl()")]),s._v("和"),a("code",[s._v("start()")]),s._v("方法即可启动爬取。")]),s._v(" "),a("p",[s._v("在"),a("code",[s._v("universal")]),s._v("中，我们新建一个"),a("code",[s._v("__init__()")]),s._v("方法，进行初始化配置，实现如下所示：")]),s._v(" "),a("div",{staticClass:"language-py line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-py"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("linkextractors "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" LinkExtractor\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" scrapy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("spiders "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" CrawlSpider"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" Rule\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" scrapyuniversal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("utils "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" get_config\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" scrapyuniversal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("rules "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" rules\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("class")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("UniversalSpider")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("CrawlSpider"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    name "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'universal'")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v("args"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("**")]),s._v("kwargs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        config "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" get_config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("config "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" config\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("rules "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" rules"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("get"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("get"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'rules'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("start_urls "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("get"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'start_urls'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("allowed_domains "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("get"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'allowed_domains'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("super")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("UniversalSpider"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("__init__"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v("args"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("**")]),s._v("kwargs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("parse_item")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        i "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" i\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br")])]),a("p",[s._v("在"),a("code",[s._v("__init__()")]),s._v("方法中，"),a("code",[s._v("start_urls")]),s._v("、"),a("code",[s._v("allowed_domains")]),s._v("、"),a("code",[s._v("rules")]),s._v("等属性被赋值。其中，"),a("code",[s._v("rules")]),s._v("属性另外读取了rules.py的配置，这样就成功实现爬虫的基础配置。")]),s._v(" "),a("p",[s._v("接下来，执行如下命令运行爬虫：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("python3 run.py china\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("程序会首先读取JSON配置文件，将配置中的一些属性赋值给Spider，然后启动爬取。运行效果完全相同，运行结果如下图所示。")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/5/21/163817bcab12be6f?imageView2/0/w/1280/h/960/format/webp/ignore-error/1",alt:""}})]),s._v(" "),a("p",[s._v("现在我们已经对Spider的基础属性实现了可配置化。剩下的解析部分同样需要实现可配置化，原来的解析函数如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("def parse_item(self, response):\n    loader = ChinaLoader(item=NewsItem(), response=response)\n    loader.add_xpath('title', '//h1[@id=\"chan_newsTitle\"]/text()')\n    loader.add_value('url', response.url)\n    loader.add_xpath('text', '//div[@id=\"chan_newsDetail\"]//text()')\n    loader.add_xpath('datetime', '//div[@id=\"chan_newsInfo\"]/text()', re='(\\d+-\\d+-\\d+\\s\\d+:\\d+:\\d+)')\n    loader.add_xpath('source', '//div[@id=\"chan_newsInfo\"]/text()', re='来源：(.*)')\n    loader.add_value('website', '中华网')\n    yield loader.load_item()\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br")])]),a("p",[s._v("我们需要将这些配置也抽离出来。这里的变量主要有Item Loader类的选用、"),a("code",[s._v("Item")]),s._v("类的选用、Item Loader方法参数的定义，我们可以在JSON文件中添加如下"),a("code",[s._v("item")]),s._v("的配置：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v('"item": {\n  "class": "NewsItem",\n  "loader": "ChinaLoader",\n  "attrs": {\n    "title": [\n      {\n        "method": "xpath",\n        "args": [\n          "//h1[@id=\'chan_newsTitle\']/text()"\n        ]\n      }\n    ],\n    "url": [\n      {\n        "method": "attr",\n        "args": [\n          "url"\n        ]\n      }\n    ],\n    "text": [\n      {\n        "method": "xpath",\n        "args": [\n          "//div[@id=\'chan_newsDetail\']//text()"\n        ]\n      }\n    ],\n    "datetime": [\n      {\n        "method": "xpath",\n        "args": [\n          "//div[@id=\'chan_newsInfo\']/text()"\n        ],\n        "re": "(\\\\d+-\\\\d+-\\\\d+\\\\s\\\\d+:\\\\d+:\\\\d+)"\n      }\n    ],\n    "source": [\n      {\n        "method": "xpath",\n        "args": [\n          "//div[@id=\'chan_newsInfo\']/text()"\n        ],\n        "re": "来源：(.*)"\n      }\n    ],\n    "website": [\n      {\n        "method": "value",\n        "args": [\n          "中华网"\n        ]\n      }\n    ]\n  }\n}\n')])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br"),a("span",{staticClass:"line-number"},[s._v("30")]),a("br"),a("span",{staticClass:"line-number"},[s._v("31")]),a("br"),a("span",{staticClass:"line-number"},[s._v("32")]),a("br"),a("span",{staticClass:"line-number"},[s._v("33")]),a("br"),a("span",{staticClass:"line-number"},[s._v("34")]),a("br"),a("span",{staticClass:"line-number"},[s._v("35")]),a("br"),a("span",{staticClass:"line-number"},[s._v("36")]),a("br"),a("span",{staticClass:"line-number"},[s._v("37")]),a("br"),a("span",{staticClass:"line-number"},[s._v("38")]),a("br"),a("span",{staticClass:"line-number"},[s._v("39")]),a("br"),a("span",{staticClass:"line-number"},[s._v("40")]),a("br"),a("span",{staticClass:"line-number"},[s._v("41")]),a("br"),a("span",{staticClass:"line-number"},[s._v("42")]),a("br"),a("span",{staticClass:"line-number"},[s._v("43")]),a("br"),a("span",{staticClass:"line-number"},[s._v("44")]),a("br"),a("span",{staticClass:"line-number"},[s._v("45")]),a("br"),a("span",{staticClass:"line-number"},[s._v("46")]),a("br"),a("span",{staticClass:"line-number"},[s._v("47")]),a("br"),a("span",{staticClass:"line-number"},[s._v("48")]),a("br"),a("span",{staticClass:"line-number"},[s._v("49")]),a("br"),a("span",{staticClass:"line-number"},[s._v("50")]),a("br"),a("span",{staticClass:"line-number"},[s._v("51")]),a("br"),a("span",{staticClass:"line-number"},[s._v("52")]),a("br"),a("span",{staticClass:"line-number"},[s._v("53")]),a("br"),a("span",{staticClass:"line-number"},[s._v("54")]),a("br"),a("span",{staticClass:"line-number"},[s._v("55")]),a("br"),a("span",{staticClass:"line-number"},[s._v("56")]),a("br")])]),a("p",[s._v("这里定义了"),a("code",[s._v("class")]),s._v("和"),a("code",[s._v("loader")]),s._v("属性，它们分别代表Item和Item Loader所使用的类。定义了"),a("code",[s._v("attrs")]),s._v("属性来定义每个字段的提取规则，例如，"),a("code",[s._v("title")]),s._v("定义的每一项都包含一个"),a("code",[s._v("method")]),s._v("属性，它代表使用的提取方法，如"),a("code",[s._v("xpath")]),s._v("即代表调用Item Loader的"),a("code",[s._v("add_xpath()")]),s._v("方法。"),a("code",[s._v("args")]),s._v("即参数，就是"),a("code",[s._v("add_xpath()")]),s._v("的第二个参数，即XPath表达式。针对"),a("code",[s._v("datetime")]),s._v("字段，我们还用了一次正则提取，所以这里还可以定义一个"),a("code",[s._v("re")]),s._v("参数来传递提取时所使用的正则表达式。")]),s._v(" "),a("p",[s._v("我们还要将这些配置之后动态加载到"),a("code",[s._v("parse_item()")]),s._v("方法里。最后，最重要的就是实现"),a("code",[s._v("parse_item()")]),s._v("方法，如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v(" def parse_item(self, response):\n    item = self.config.get('item')\n    if item:\n        cls = eval(item.get('class'))()\n        loader = eval(item.get('loader'))(cls, response=response)\n        # 动态获取属性配置\n        for key, value in item.get('attrs').items():\n            for extractor in value:\n                if extractor.get('method') == 'xpath':\n                    loader.add_xpath(key, *extractor.get('args'), **{'re': extractor.get('re')})\n                if extractor.get('method') == 'css':\n                    loader.add_css(key, *extractor.get('args'), **{'re': extractor.get('re')})\n                if extractor.get('method') == 'value':\n                    loader.add_value(key, *extractor.get('args'), **{'re': extractor.get('re')})\n                if extractor.get('method') == 'attr':\n                    loader.add_value(key, getattr(response, *extractor.get('args')))\n        yield loader.load_item()\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br")])]),a("p",[s._v("这里首先获取Item的配置信息，然后获取"),a("code",[s._v("class")]),s._v("的配置，将其初始化，初始化Item Loader，遍历Item的各个属性依次进行提取。判断"),a("code",[s._v("method")]),s._v("字段，调用对应的处理方法进行处理。如"),a("code",[s._v("method")]),s._v("为"),a("code",[s._v("css")]),s._v("，就调用Item Loader的"),a("code",[s._v("add_css()")]),s._v("方法进行提取。所有配置动态加载完毕之后，调用"),a("code",[s._v("load_item()")]),s._v("方法将Item提取出来。")]),s._v(" "),a("p",[s._v("重新运行程序，结果如下图所示。")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://user-gold-cdn.xitu.io/2018/5/21/163817bcab9661d0?imageView2/0/w/1280/h/960/format/webp/ignore-error/1",alt:""}})]),s._v(" "),a("p",[s._v("运行结果是完全相同的。")]),s._v(" "),a("p",[s._v("我们再回过头看一下"),a("code",[s._v("start_urls")]),s._v("的配置。这里"),a("code",[s._v("start_urls")]),s._v("只可以配置具体的链接。如果这些链接有100个、1000个，我们总不能将所有的链接全部列出来吧？在某些情况下，"),a("code",[s._v("start_urls")]),s._v("也需要动态配置。我们将"),a("code",[s._v("start_urls")]),s._v("分成两种，一种是直接配置URL列表，一种是调用方法生成，它们分别定义为"),a("code",[s._v("static")]),s._v("和"),a("code",[s._v("dynamic")]),s._v("类型。")]),s._v(" "),a("p",[s._v("本例中的"),a("code",[s._v("start_urls")]),s._v("很明显是"),a("code",[s._v("static")]),s._v("类型的，所以"),a("code",[s._v("start_urls")]),s._v("配置改写如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v('"start_urls": {\n  "type": "static",\n  "value": [\n    "http://tech.china.com/articles/"\n  ]\n}\n')])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br")])]),a("p",[s._v("如果"),a("code",[s._v("start_urls")]),s._v("是动态生成的，我们可以调用方法传参数，如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v('"start_urls": {\n  "type": "dynamic",\n  "method": "china",\n  "args": [\n    5, 10\n  ]\n}\n')])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br")])]),a("p",[s._v("这里"),a("code",[s._v("start_urls")]),s._v("定义为"),a("code",[s._v("dynamic")]),s._v("类型，指定方法为"),a("code",[s._v("urls_china()")]),s._v("，然后传入参数5和10，来生成第5到10页的链接。这样我们只需要实现该方法即可，统一新建一个urls.py文件，如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("def china(start, end):\n    for page in range(start, end + 1):\n        yield 'http://tech.china.com/articles/index_' + str(page) + '.html'\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[s._v("其他站点可以自行配置。如某些链接需要用到时间戳，加密参数等，均可通过自定义方法实现。")]),s._v(" "),a("p",[s._v("接下来在Spider的"),a("code",[s._v("__init__()")]),s._v("方法中，"),a("code",[s._v("start_urls")]),s._v("的配置改写如下所示：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("from scrapyuniversal import urls\n\nstart_urls = config.get('start_urls')\nif start_urls:\n    if start_urls.get('type') == 'static':\n        self.start_urls = start_urls.get('value')\n    elif start_urls.get('type') == 'dynamic':\n        self.start_urls = list(eval('urls.' + start_urls.get('method'))(*start_urls.get('args', [])))\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br")])]),a("p",[s._v("这里通过判定"),a("code",[s._v("start_urls")]),s._v("的类型分别进行不同的处理，这样我们就可以实现"),a("code",[s._v("start_urls")]),s._v("的配置了。")]),s._v(" "),a("p",[s._v("至此，Spider的设置、起始链接、属性、提取方法都已经实现了全部的可配置化。")]),s._v(" "),a("p",[s._v("综上所述，整个项目的配置包括如下内容。")]),s._v(" "),a("ul",[a("li",[a("p",[a("strong",[a("code",[s._v("spider")])]),s._v("：指定所使用的Spider的名称。")])]),s._v(" "),a("li",[a("p",[a("strong",[a("code",[s._v("settings")])]),s._v("：可以专门为Spider定制配置信息，会覆盖项目级别的配置。")])]),s._v(" "),a("li",[a("p",[a("strong",[a("code",[s._v("start_urls")])]),s._v("：指定爬虫爬取的起始链接。")])]),s._v(" "),a("li",[a("p",[a("strong",[a("code",[s._v("allowed_domains")])]),s._v("：允许爬取的站点。")])]),s._v(" "),a("li",[a("p",[a("strong",[a("code",[s._v("rules")])]),s._v("：站点的爬取规则。")])]),s._v(" "),a("li",[a("p",[a("strong",[a("code",[s._v("item")])]),s._v("：数据的提取规则。")])])]),s._v(" "),a("p",[s._v("我们实现了Scrapy的通用爬虫，每个站点只需要修改JSON文件即可实现自由配置。")]),s._v(" "),a("h2",{attrs:{id:"八、本节代码"}},[s._v("八、本节代码")]),s._v(" "),a("p",[s._v("本节代码地址为：https://github.com/Python3WebSpider/ScrapyUniversal。")]),s._v(" "),a("h2",{attrs:{id:"九、结语"}},[s._v("九、结语")]),s._v(" "),a("p",[s._v("本节介绍了Scrapy通用爬虫的实现。我们将所有配置抽离出来，每增加一个爬虫，就只需要增加一个JSON文件配置。之后我们只需要维护这些配置文件即可。如果要更加方便的管理，可以将规则存入数据库，再对接可视化管理页面即可。")]),s._v(" "),a("p",[s._v("本资源首发于崔庆才的个人博客静觅： "),a("a",{attrs:{href:"https://link.juejin.im/?target=https%3A%2F%2Flink.juejin.im%2F%3Ftarget%3Dhttps%253A%252F%252Flink.juejin.im%252F%253Ftarget%253Dhttps%25253A%25252F%25252Flink.juejin.im%25252F%25253Ftarget%25253Dhttps%2525253A%2525252F%2525252Flink.juejin.im%2525252F%2525253Ftarget%2525253Dhttps%252525253A%252525252F%252525252Flink.juejin.im%252525252F%252525253Ftarget%252525253Dhttps%25252525253A%25252525252F%25252525252Flink.juejin.im%25252525252F%25252525253Ftarget%25252525253Dhttps%2525252525253A%2525252525252F%2525252525252Flink.zhihu.com%2525252525252F%2525252525253Ftarget%2525252525253Dhttps%252525252525253A%2525252525252F%2525252525252Fcuiqingcai.com%2525252525252F5052.html",target:"_blank",rel:"noopener noreferrer"}},[s._v("Python3网络爬虫开发实战教程 | 静觅"),a("OutboundLink")],1)]),s._v(" "),a("p",[s._v("如想了解更多爬虫资讯，请关注我的个人微信公众号：进击的Coder")])])}),[],!1,null,null,null);t.default=n.exports}}]);